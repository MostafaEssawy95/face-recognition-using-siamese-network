{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25eee58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_snippets import *\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import random\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb10c0",
   "metadata": {},
   "source": [
    "# define the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90d209aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class siamese_network (Dataset):\n",
    "    def __init__ (self,folder,transform=None,should_invert=True):\n",
    "        self.folder=folder\n",
    "        self.transform=transform\n",
    "        self.items=Glob(f'{self.folder}/*/*')\n",
    "        \n",
    "    def __getitem__(self,ix):\n",
    "        itemA=self.items[ix]\n",
    "        person=str(self.items[ix]).split('\\\\')[-2]\n",
    "        same_person = randint(2)\n",
    "        if same_person:\n",
    "            rando=random.randint(1,10)\n",
    "            itemB=os.path.join(self.folder,person,'{}.png'.format(str(rando)))\n",
    "        else: \n",
    "            while True:\n",
    "                itemB=choose(self.items)\n",
    "                if person != (str(itemB).split('\\\\')[-2]):\n",
    "                    break\n",
    "                    \n",
    "        imgA=read(itemA)\n",
    "        imgB=read(itemB)\n",
    "        if self.transform:\n",
    "            imgA=self.transform(imgA)\n",
    "            imgB=self.transform(imgB)\n",
    "            \n",
    "        return imgA, imgB, np.array([1-same_person])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e9203",
   "metadata": {},
   "source": [
    "# define transformations needed in training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "713d8c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "trn_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(5, (0.01,0.2),\n",
    "                            scale=(0.9,1.1)),\n",
    "    transforms.Resize((100,100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((100,100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0267aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e1685eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 23:34:02.019 | INFO     | torch_snippets.paths:inner:24 - 370 files found at C:\\Users\\mosta\\Desktop\\png faces\\all_images\\training/*/*\n",
      "2021-12-10 23:34:02.022 | INFO     | torch_snippets.paths:inner:24 - 30 files found at C:\\Users\\mosta\\Desktop\\png faces\\all_images\\testing/*/*\n"
     ]
    }
   ],
   "source": [
    "trn_ds = siamese_network (folder=r\"C:\\Users\\mosta\\Desktop\\png faces\\all_images\\training\", transform=trn_tfms)\n",
    "val_ds = siamese_network (folder=r\"C:\\Users\\mosta\\Desktop\\png faces\\all_images\\testing\", transform=val_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959bc44f",
   "metadata": {},
   "source": [
    "# make dataloader for both datasets with batch size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c77ac32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(trn_ds, shuffle=True, batch_size=64)\n",
    "val_dl = DataLoader(val_ds, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930dfb37",
   "metadata": {},
   "source": [
    "# define convolution block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08844b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convBlock(ni, no):\n",
    "    return nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Conv2d(ni, no, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(no),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbc609",
   "metadata": {},
   "source": [
    "# define architecture of siamese network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e528a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            convBlock(1,4),\n",
    "            convBlock(4,12),\n",
    "            convBlock(12,16),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*100*100,500), nn.ReLU(inplace=True),\n",
    "            nn.Linear(500,500), nn.ReLU(inplace=True),\n",
    "            nn.Linear(500,500), nn.ReLU(inplace=True),\n",
    "            nn.Linear(500,5)\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.features(input1)\n",
    "        output2 = self.features(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6daa54",
   "metadata": {},
   "source": [
    "# define contrastive loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52ea76c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2,keepdim=True)\n",
    "        pos = (1-label) * torch.pow(euclidean_distance, 2)\n",
    "        neg = (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        loss_contrastive = torch.mean( pos + neg )\n",
    "        acc = ((euclidean_distance>0.6)==label).float().mean()\n",
    "        return loss_contrastive,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc536c71",
   "metadata": {},
   "source": [
    "# define training batch function to get loss and accuracy at each batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f8acb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, data, optimizer, criterion):\n",
    "    imgsA, imgsB, labels = [t.to(device) for t in data]\n",
    "    optimizer.zero_grad()\n",
    "    codesA, codesB = model(imgsA, imgsB)\n",
    "    loss,acc = criterion(codesA, codesB, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(),acc.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_batch(model, data, criterion):\n",
    "    imgsA, imgsB, labels = [t.to(device) for t in data]\n",
    "    codesA, codesB = model(imgsA, imgsB)\n",
    "    loss,acc = criterion(codesA, codesB, labels)\n",
    "    return loss.item(),acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0419d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0dc24a",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "528403ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20.000\ttrn_loss: 0.469\ttrn_acc: 0.695\tval_loss: 0.245\tval_acc: 0.867\t(25.08s - 100.34s remaining)\n",
      "EPOCH: 40.000\ttrn_loss: 0.352\ttrn_acc: 0.776\tval_loss: 0.150\tval_acc: 0.900\t(50.74s - 76.12s remaining)\n",
      "EPOCH: 60.000\ttrn_loss: 0.281\ttrn_acc: 0.867\tval_loss: 0.097\tval_acc: 1.000\t(76.46s - 50.97s remaining)\n",
      "EPOCH: 80.000\ttrn_loss: 0.258\ttrn_acc: 0.864\tval_loss: 0.109\tval_acc: 0.900\t(101.52s - 25.38s remaining)\n",
      "EPOCH: 100.000\ttrn_loss: 0.196\ttrn_acc: 0.931\tval_loss: 0.057\tval_acc: 0.967\t(127.03s - 0.00s remaining)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_epochs = 100\n",
    "log = Report(n_epochs)\n",
    "for epoch in range(n_epochs):\n",
    "    N = len(trn_dl)\n",
    "    for i, data in enumerate(trn_dl):\n",
    "        loss,acc= train_batch(model, data, optimizer, criterion)\n",
    "        log.record(epoch+(1+i)/N, trn_loss=loss, trn_acc=acc, end='\\r')\n",
    "     \n",
    "    N = len(val_dl)\n",
    "    for i, data in enumerate(val_dl):\n",
    "        loss,acc= validate_batch(model, data, criterion) \n",
    "        log.record(epoch+(1+i)/N, val_loss=loss, val_acc=acc, end='\\r')\n",
    "    if (epoch+1)%20==0: log.report_avgs(epoch+1)\n",
    "    if epoch==10: optimizer = optim.Adam(model.parameters(), lr=0.0005)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
